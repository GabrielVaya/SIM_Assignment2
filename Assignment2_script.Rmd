---
title: "Untitled"
author: "Gabriel Vayá"
date: "2023-12-13"
output: 
  pdf_document: 
    toc: true
    toc_depth: 3
    number_sections: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

library(car)
library(MASS)
library(missMDA)
library(visdat)
library(FactoMineR)
library(chemometrics)
library(corrplot)
library(naniar)
library(nortest)
library(visdat)
library(tidyverse)
library(lsr)
library(effects)
```

First thing will be to load the data into R, and redeclaring the variables to properly comply with the needs of the analysis. Notice we relable the levels of the variable SeniorCitizen from (0,1) to (No, Yes) for practical reasons. 
```{r}
#setwd("C:\\Users\\darry\\Documents\\MDS\\Statistical_Inference_And_Modelling\\SIM_Assignment2")
#setwd("/Users/gabrielvayaabad/Desktop/MDS/SIM/Assignment 2")
setwd("C:/Users/Admin/Desktop/MÀSTER DATA SCIENCE/SIM/Assigment 2")
#setwd("D:/MDS/ADSDB/SIM_Assignment2")


df$MonthlyCharges <- as.numeric(df$MonthlyCharges)
df$TotalCharges <- as.numeric(df$TotalCharges)
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
df$SeniorCitizen <- as.factor(df$SeniorCitizen)
df$customerID <- as.character(df$customerID)
levels(df$SeniorCitizen) <- c("No","Yes")
```


#DataPreparation

#Deduplication
Now we check for duplicates in the dataset, and we see all rows are distinct. 
```{r}
dup <- which(duplicated(df)) #No duplicate rows
```

#Missing Values
For the missing data, we can...
```{r}
vis_miss(df)
df[is.na(df$TotalCharges),] #NAs only found in TotalCharges variable

mcar_test(df) #p-value is 0 -> not random
df$tenure[is.na(df$TotalCharges)] #TotalCharges are NA when tenure is 0
df$TotalCharges[is.na(df$TotalCharges)] <- 0 #impute with 0
```

#Deduplication (again)
```{r}
dup <- which(duplicated(df)) #No duplicate rows
```

#Looking for errors
```{r}
df.aux <- df
df.aux$TheoreticalTotalCharges <- df.aux$tenure*df.aux$MonthlyCharges
df.aux[df.aux$TheoreticalTotalCharges > df.aux$TotalCharges,]
```


#EDA (+ Univariate Outliers)

#gender
```{r}
na.gender <- sum(is.na(df$gender)) #No NAs
barplot(table(df$gender),col='lightblue') #Balanced
```

#SeniorCitizen
```{r}
na.seniorcitizen <- sum(is.na(df$SeniorCitizen)) #No NAs
barplot(table(df$SeniorCitizen),col='lightblue') #Unbalanced
```

#Partner
```{r}
na.partner <- sum(is.na(df$Partner)) #No NAs
barplot(table(df$Partner),col='lightblue') #Balanced
```

#Dependents
```{r}
na.dependents <- sum(is.na(df$Dependents)) #No NAs
barplot(table(df$Dependents),col='lightblue') #Unbalanced
```

#tenure
```{r}
na.tenure <- sum(is.na(df$tenure)) #No NAs
hist(df$tenure,freq=F,15) #Young customers overrepresented
mm <- mean(df$tenure,na.rm=T);ss <- sd(df$tenure,na.rm=T);
curve(dnorm(x,mm,ss),col="red",add=T)
#shapiro.test(df$tenure) #Error: too many samples for shapiro test
ad.test(df$tenure) #Anderson-Darling test: Not normally distributed
Boxplot(df$tenure,range=1.5,id=list(n=Inf,labels=rownames(df))) #No mild univariate outliers
Boxplot(df$tenure,range=3,id=list(n=Inf,labels=rownames(df))) #No extreme univariate outliers
```

#PhoneService
```{r}
na.phoneservice <- sum(is.na(df$PhoneService)) #No NAs
barplot(table(df$PhoneService),col='lightblue') #Unbalanced
```

#MultipleLines
```{r}
na.multiplelines <- sum(is.na(df$MultipleLines)) #No NAs
barplot(table(df$MultipleLines),col='lightblue') #Unbalanced in "No phone service"
```

#InternetService
```{r}
na.internetservice <- sum(is.na(df$InternetService)) #No NAs
barplot(table(df$InternetService),col='lightblue') #Unbalanced
```

#OnlineSecurity
```{r}
no.onlinesecurity <- sum(is.na(df$OnlineSecurity)) #No NAs
barplot(table(df$OnlineSecurity),col='lightblue') #Unbalanced in "No"
```

#OnlineBackup
```{r}
na.onlinebackup <- sum(is.na(df$OnlineBackup)) #No NAs
barplot(table(df$OnlineBackup),col='lightblue') #Unbalanced
```

#DeviceProtection
```{r}
na.deviceprotection <- sum(is.na(df$DeviceProtection)) #No NAs
barplot(table(df$DeviceProtection),col='lightblue') #Unbalanced
```

#TechSupport
```{r}
na.techsupport <- sum(is.na(df$TechSupport)) #No NAs
barplot(table(df$TechSupport),col='lightblue') #Unbalanced in "No"
```

#StreamingMovies
```{r}
na.streamingmovies <- sum(is.na(df$StreamingMovies)) #No NAs
barplot(table(df$StreamingMovies),col='lightblue') #Unbalanced in "No internet service"
```

#Contract
```{r}
na.contract <- sum(is.na(df$Contract)) #No NAs
barplot(table(df$Contract),col='lightblue') #Unbalanced in "Month-to-month"
```

#PaperlessBilling
```{r}
na.paperlessbilling <- sum(is.na(df$PaperlessBilling)) #No NAs
barplot(table(df$PaperlessBilling),col='lightblue') #Relatively balanced
```

#PaymentMethod
```{r}
na.paymentmethod <- sum(is.na(df$PaymentMethod)) #No NAs
barplot(table(df$PaymentMethod),col='lightblue') #Unbalanced in "Credit card (automatic)"
```

#MonthlyCharges
```{r}
na.monthlycharges <- sum(is.na(df$MonthlyCharges)) #No NAs
hist(df$MonthlyCharges,freq=F,15)
mm <- mean(df$MonthlyCharges,na.rm=T)
ss <- sd(df$MonthlyCharges,na.rm=T)
curve(dnorm(x,mm,ss),col="red",add=T)
#shapiro.test(df$MonthlyCharges) #Error: too many samples for shapiro test
ad.test(df$MonthlyCharges) #Anderson-Darling test: Not normally distributed
Boxplot(df$MonthlyCharges,range=1.5,id=list(n=Inf,labels=rownames(df))) #No mild univariate outliers
Boxplot(df$MonthlyCharges,range=3,id=list(n=Inf,labels=rownames(df))) #No severe univariate outliers
```

#TotalCharges
```{r}
na.totalcharges <- sum(is.na(df$TotalCharges)) #11 NAs imputed earlier
hist(df$TotalCharges,freq=F,15)
mm <- mean(df$TotalCharges,na.rm=T)
ss <- sd(df$TotalCharges,na.rm=T)
curve(dnorm(x,mm,ss),col="red",add=T)
#shapiro.test(df$TotalCharges) #Error: too many samples for shapiro test
ad.test(df$TotalCharges) #Anderson-Darling test: Not normally distributed
Boxplot(df$TotalCharges,range=1.5,id=list(n=Inf,labels=rownames(df))) #No mild univariate outliers
Boxplot(df$TotalCharges,range=3,id=list(n=Inf,labels=rownames(df))) #No severe univariate outliers
```

#Target Variable: Churn
```{r}
na.churn <- sum(is.na(df$Churn)) #No NAs
barplot(table(df$Churn),col='lightblue') #Unbalanced
res.cat <- catdes(df,21)
res.cat$category
```

#Correlations and Associations
```{r}
num <- which(sapply(df,is.numeric))

# Correlations
vis_dat(df[,num], sort_type = FALSE)
vis_cor(df[,num])

# Mixed Associations (using Chisquared pvalue and CramersV)
cat <- which(sapply(df, function(x) is.factor(x) || is.character(x)))

# function to get chi square p value and Cramers V
f = function(x,y) {
    tbl = df %>% select(x,y) %>% table()
    chisq_pval = round(chisq.test(tbl)$p.value, 4)
    cramV = round(cramersV(tbl), 4) 
    data.frame(x, y, chisq_pval, cramV) }

# create unique combinations of column names
# sorting will help getting a better plot (upper triangular)
df_comb = data.frame(t(combn(sort(names(df)), 2)), stringsAsFactors = F)

# apply function to each variable combination
df_res = map2_df(df_comb$X1, df_comb$X2, f)

# plot results
df_res %>%
  ggplot(aes(x,y,fill=chisq_pval))+
  geom_tile()+
  geom_text(aes(x,y,label=cramV), size=2)+
  scale_fill_gradient(low="red", high="yellow")+
  theme_classic()+
  theme(axis.text.x = element_text(angle = 30, hjust = 1))

# Function to find mixed associations found at:
# https://stackoverflow.com/questions/52554336/plot-the-equivalent-of-correlation-matrix-for-factors-categorical-data-and-mi
# By AntoniosK on StackOverflow
```

#Multivariate Outliers
```{r}
res.mout <- Moutlier(df[,num],quantile=0.99,plot=F)
length(which(res.mout$md > res.mout$cutoff))
mout <- which(res.mout$md > res.mout$cutoff)
summary(df[mout, num]) #Summary of outliers
plot(res.mout$md, res.mout$rd)

#df <- df[-m.out,] #remove multivariate outliers
```
Maybe because there is continuity in the distribution we don't take them out (see plot)

#Profiling of target variable 
```{r}
summary(df$Churn)
ptt<-prop.table(table(df$Churn));ptt
catdes(df,21)
```


#Modelling
Now we enter into the modelling stage of the analysis. We first want to construct a robust numerical model, in order to add transformations in the numerical variables. Afterwards we will add the main categorical variables into this best numerical model, and finally we will look at interactions between variables to make the final model.
#Model with numerical variables
We only have 3 numerical variables, so, in the first place, we construct a model with all 3 numerical variables: tenure, MonthlyCharges and TotalCharges. We are suspicious of multicolinearity regarding TotalCharges, since the number calculated by multiplying the tenure months by MonthlyCharges gives a similar result than the TotalCharges value (with small a deviation probably coming from opening fees or discounts in the different contracts). (...)
```{r}
attach(df)
nm1 <- glm(Churn ~ tenure + MonthlyCharges + TotalCharges, family="binomial", data = df)
summary(nm1)
vif.nm1 <- vif(nm1);vif.nm1
step(nm1,k= log(nrow(df))) 
```

```{r}
nm2 <- glm(Churn ~ tenure + MonthlyCharges, family="binomial", data = df)
summary(nm2)
anova(nm2,nm1,test="Chisq") #p-value: 0.02277 -> we reject at 95% confidence that the variance explained is the same. We don't at 99% confidence. 
```


#Transformations of numerical variables
Firstly, we examine marginalModelPlots to gain insights into which variables effectively fit the model. Subsequently, we observed the necessity for a transformation in the tenure variable.
A reduction of one unit in tenure is associated with a log-odds increase of -0.054850 for Churn. Consequently, implementing a Square Root Transformation becomes imperative.
```{r}
nm3 <- glm(Churn ~ tenure+ I(tenure^2) + MonthlyCharges, family="binomial", data = df)
summary(nm3)
marginalModelPlots(nm3)

nm4 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges, family="binomial", data = df)
summary(nm4)
marginalModelPlots(nm4)
```

#Adding main categorical effects
Now we attempt to add our factor variables to the model. First we are building an inicial model with the main numerical variables and all the categorical ones, which will obviously result in a too complex model to be analysed or properly interpreted. After that, we are conducting an Anova Chisq test to assess the significance of each categorical variable. 
```{r}
cm1 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges + gender + SeniorCitizen + Partner + Dependents + PhoneService + MultipleLines + InternetService + OnlineSecurity + OnlineBackup + DeviceProtection + TechSupport + StreamingTV + StreamingMovies + Contract + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(cm1)
Anova(cm1,test="LR")
```
This Anova tests suggests, at 95% confidence, to take out of the modeling the following variables: gender, Partner, Dependets, PhoneService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV and StreamingMovies. Now we construct a model only with the significant variables.
```{r}
cm2 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(cm2)
```


#Interactions
Significant interactions (5% level):
poly(tenure, 2):Contract
MonthlyCharges:MultipleLines
MonthlyCharges:InternetService
MonthlyCharges:Contract
SeniorCitizen:PaymentMethod
InternetService:Contract
InternetService:PaymentMethod
```{r}
Anova(glm(Churn ~ (poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod) * (poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod), family="binomial", data = df))
```
We will be adding interactions one by one to our model, and we will see if they have significance over the last model, and, if they do, we keep them in the model and move on. Note that to do that we will be using Fisher tests, with the null hypothesis being that the variance explained by both models are the same. 
First we look into the interaction between Contract and the transformed tenure. We can see that the role it performs on the model is to ... . Then, we perform an Fisher test to see if the variances explained by the two models are the same or not. 
```{r}
gm1 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm1)
anova(cm2,gm1,test="Chisq") #p-value 3.435e-06
```
We can see that the p-value in this case is 3.435e-06, so we reject the null hypothesis and we decide to keep the interaction forward. 
Now let's look at the interactions regarding MonthlyCharges, starting by its interaction with MultipleLines.
```{r}
gm2 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*MultipleLines + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm2)
anova(gm1,gm2,test="Chisq") #p-value 0.01332
```
In this case, the p-value is 0.01332, which would make us reject the hypothesis at 99% confidence, but accept it at 95%. We decide to take it out of the model.
```{r}
gm3 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm3)
anova(gm1,gm3,test="Chisq") #p-value 0.007456 
```
This time, the p-value is 0.007456, so we will keep the interaction in the model.
```{r}
gm4 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*(InternetService + Contract) + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm4)
anova(gm3,gm4,test="Chisq") #p-value 0.6995 
```
Now the p-value is 0.6995, so we accept the null hypothesis and not keep this interaction.
```{r}
gm5 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen*PaymentMethod + MultipleLines + InternetService + PaperlessBilling , family="binomial", data = df)
summary(gm5)
anova(gm3,gm5,test="Chisq") #p-value 0.03849  
```
In this final case, we could reject the null hypothesis at 95% confidence, but we decide to take this interaction  out of the model.

#Train-test validation & final interpretation
```{r}
fm <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(fm)
Anova(fm)

```

Assesment
```{r}
Boxplot(hatvalues(fm))
abline(h=4*length(coef(fm))/nrow(df))

#observations with cooks distance very large are dangerous
Boxplot(cooks.distance(fm), id.method=list(label=rownames(df))) #2 obs far away that should be removed from the database because are influent data outliers according to cooks distance (3972 and 5948)

influencePlot(fm)  #according to hat values 2372 and 3668 are too large

llcoo <- c(3972, 5948, 2372, 3668)
df <- df[-llcoo,]

```


Test-train
```{r}
set.seed(1234)
llwork <- sample(1:nrow(df),round(0.8*nrow(df),dig=0))
dfwork <- df[llwork,]
dftest <- df[-llwork,]

fm <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = dfwork)

m15 <- step(fm, k=log(nrow(dfwork)))
Anova(m15, test="LR") #make an assement for residual deviance and how it affects degrees of freedom

summary(m15) #goodness of fitt asses deviance (model) vs df(model)

library(DescTools)
PseudoR2(m15, "all")#0.28 -> good model

predprob <- predict(m15, type="response")
summary(predprob)
```


split sample
```{r}

predprob <- predict(m15, type="response")
summary(predprob)
summary(fitted(m15))
hoslem.test(dfwork$pres, fitted(m15) ) 

```

# Classical: confusion tables
```{r}
predprob <- predict(m15, type="response")
predi.est <- ifelse(predprob<0.5, 0, 1)
table(predi.est, dfwork$pres)

#Accuracy 
```


