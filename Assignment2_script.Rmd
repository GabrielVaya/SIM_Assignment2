---
title: "Untitled"
author: "Gabriel Vayá"
date: "2023-12-13"
output: 
  pdf_document: 
    toc: true
    toc_depth: 3
    number_sections: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 4)
options(warn=-1)
```

```{r}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

library(car)
library(MASS)
library(missMDA)
library(visdat)
library(FactoMineR)
library(chemometrics)
library(corrplot)
library(naniar)
library(nortest)
library(visdat)
library(tidyverse)
library(lsr)
library(effects)
```

First thing will be to load the data into R, and redeclaring the variables to properly comply with the needs of the analysis. Notice we relable the levels of the variable SeniorCitizen from (0,1) to (No, Yes) for practical reasons. 
```{r}
setwd("C:\\Users\\darry\\Documents\\MDS\\Statistical_Inference_And_Modelling\\SIM_Assignment2")
#setwd("/Users/gabrielvayaabad/Desktop/MDS/SIM/Assignment 2")
#setwd("C:/Users/Admin/Desktop/MÀSTER DATA SCIENCE/SIM/Assigment 2")
#setwd("D:/MDS/ADSDB/SIM_Assignment2")

df <- read.csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
#df <- WA_Fn_UseC_Telco_Customer_Churn

df$MonthlyCharges <- as.numeric(df$MonthlyCharges)
df$TotalCharges <- as.numeric(df$TotalCharges)
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
df$SeniorCitizen <- as.factor(df$SeniorCitizen)
df$customerID <- as.character(df$customerID)
levels(df$SeniorCitizen) <- c("No","Yes")
```


# DataPreparation  

#  Missing Values  
For the missing data, we see that there are only 11 missing values. If we look even closer, we can see they are all in the same variable, TotalCharges. Analysing even further, we can see those missing values are corresponding to those clients with 0 month of tenure. Hence are imputing them with 0. 
```{r}
vis_miss(df)
#df[is.na(df$TotalCharges),] #NAs only found in TotalCharges variable

mcar_test(df) #p-value is 0 -> not random
df$tenure[is.na(df$TotalCharges)] #TotalCharges are NA when tenure is 0
df$TotalCharges[is.na(df$TotalCharges)] <- 0 #impute with 0
```

#  Deduplication 
Once imputed, we look into the duplicate rows. 
```{r}
dup <- which(duplicated(df)) #No duplicate rows
```

#  Looking for errors
In the errors, we want to see, first of all we look for huge discrepancies between the total charges, and the product between MonthlyCharges and tenure months. We see some, but small, probably due to discounts or opening fees. Next, we want to see if the number of clients without phone service is the same throughout the variables. It is, 682. Finally, we do the same with internet service, and we see that it is the same, 1526. 
```{r}
df.aux <- df
df.aux$TheoreticalTotalCharges <- df.aux$tenure*df.aux$MonthlyCharges
#df.aux[df.aux$TheoreticalTotalCharges > df.aux$TotalCharges,] #uncomment to see full table

#Look at phone service
table(df.aux$PhoneService)
table(df.aux$MultipleLines)

#Look at internet service
table(df.aux$OnlineSecurity)
table(df.aux$OnlineBackup)
table(df.aux$InternetService)
table(df.aux$TechSupport)
table(df.aux$StreamingTV)
table(df.aux$StreamingMovies)
```


# EDA (+ Univariate Outliers)  

#  gender  
This is the gender of the customer, which as we can see is balanced throughout the dataset.
```{r}
na.gender <- sum(is.na(df$gender)) #No NAs
barplot(table(df$gender),col='lightblue') #Balanced
```

#  SeniorCitizen  
As far as SeniorCitizen, there is much less SeniorCitizens as you can imagine.
```{r}
na.seniorcitizen <- sum(is.na(df$SeniorCitizen)) #No NAs
barplot(table(df$SeniorCitizen),col='lightblue') #Unbalanced
```

#  Partner  
Those are customers with partner, and we see it is pretty balanced.
```{r}
na.partner <- sum(is.na(df$Partner)) #No NAs
barplot(table(df$Partner),col='lightblue') #Balanced
```

#  Dependents  
The dependent customers follow a similar distribution than the SeniorCitizen.
```{r}
na.dependents <- sum(is.na(df$Dependents)) #No NAs
barplot(table(df$Dependents),col='lightblue') #Unbalanced
```

#  tenure  
This is the months that a customers have been in the company. We can see that the majority of the customer base are recent customers. We can see that, using an Anderson-Darling test the variable is not normally distributied, and has no univariate outliers.
```{r}
na.tenure <- sum(is.na(df$tenure)) #No NAs
hist(df$tenure,freq=F,15) #Young customers overrepresented
mm <- mean(df$tenure,na.rm=T);ss <- sd(df$tenure,na.rm=T);
curve(dnorm(x,mm,ss),col="red",add=T)
#shapiro.test(df$tenure) #Error: too many samples for shapiro test
ad.test(df$tenure) #Anderson-Darling test: Not normally distributed
Boxplot(df$tenure,range=1.5,id=list(n=Inf,labels=rownames(df))) #No mild univariate outliers
Boxplot(df$tenure,range=3,id=list(n=Inf,labels=rownames(df))) #No extreme univariate outliers
```

#  PhoneService  
Now we can see that the majority of the clients have PhoneService in the contract.
```{r}
na.phoneservice <- sum(is.na(df$PhoneService)) #No NAs
barplot(table(df$PhoneService),col='lightblue') #Unbalanced
```

#  MultipleLines  
As we saw before, the majority of the clients have PhoneService, so they are underrepresented in this variable, but regarding those who have, MultipleLines is pretty balanced.
```{r}
na.multiplelines <- sum(is.na(df$MultipleLines)) #No NAs
barplot(table(df$MultipleLines),col='lightblue') #Unbalanced in "No phone service"
```

#  InternetService  
Most of the clients have fiber optic, and there are a few that don't have internet service.
```{r}
na.internetservice <- sum(is.na(df$InternetService)) #No NAs
barplot(table(df$InternetService),col='lightblue') #Unbalanced
```

#  OnlineSecurity  
Those clients without internet service are a minority as they were in the previous variable, and most of those with internet service don't have OnlineSecurity.
```{r}
no.onlinesecurity <- sum(is.na(df$OnlineSecurity)) #No NAs
barplot(table(df$OnlineSecurity),col='lightblue') #Unbalanced in "No"
```

#  OnlineBackup  
Similar to previous variable
```{r}
na.onlinebackup <- sum(is.na(df$OnlineBackup)) #No NAs
barplot(table(df$OnlineBackup),col='lightblue') #Unbalanced
```

#  DeviceProtection  
Similar to previous variable
```{r}
na.deviceprotection <- sum(is.na(df$DeviceProtection)) #No NAs
barplot(table(df$DeviceProtection),col='lightblue') #Unbalanced
```

#  TechSupport  
Similar to previous variable
```{r}
na.techsupport <- sum(is.na(df$TechSupport)) #No NAs
barplot(table(df$TechSupport),col='lightblue') #Unbalanced in "No"
```

#  StreamingMovies  
In this case, disregarding the users without internet service, the categories are balanced.
```{r}
na.streamingmovies <- sum(is.na(df$StreamingMovies)) #No NAs
barplot(table(df$StreamingMovies),col='lightblue') #Unbalanced in "No internet service"
```

#  Contract  
Most of the contracts are month to month.
```{r}
na.contract <- sum(is.na(df$Contract)) #No NAs
barplot(table(df$Contract),col='lightblue') #Unbalanced in "Month-to-month"
```

#  PaperlessBilling  
This variable is pretty much balanced, with more population towards having PaperlessBilling.
```{r}
na.paperlessbilling <- sum(is.na(df$PaperlessBilling)) #No NAs
barplot(table(df$PaperlessBilling),col='lightblue') #Relatively balanced
```

#  PaymentMethod  
Most of the customers use electronic check.
```{r}
na.paymentmethod <- sum(is.na(df$PaymentMethod)) #No NAs
barplot(table(df$PaymentMethod),col='lightblue') #Unbalanced in "Electronic check"
```

#  MonthlyCharges  
We can see that most of population concentrates around 20 units of MothlyCharges. Not normally distributed. No univariate outliers. 
```{r}
na.monthlycharges <- sum(is.na(df$MonthlyCharges)) #No NAs
hist(df$MonthlyCharges,freq=F,15)
mm <- mean(df$MonthlyCharges,na.rm=T)
ss <- sd(df$MonthlyCharges,na.rm=T)
curve(dnorm(x,mm,ss),col="red",add=T)
#shapiro.test(df$MonthlyCharges) #Error: too many samples for shapiro test
ad.test(df$MonthlyCharges) #Anderson-Darling test: Not normally distributed
Boxplot(df$MonthlyCharges,range=1.5,id=list(n=Inf,labels=rownames(df))) #No mild univariate outliers
Boxplot(df$MonthlyCharges,range=3,id=list(n=Inf,labels=rownames(df))) #No severe univariate outliers
```

#  TotalCharges  
Again, most of the population groups around small values. Not normally distributed. No univariate outliers.  
```{r}
na.totalcharges <- sum(is.na(df$TotalCharges)) #11 NAs imputed earlier
hist(df$TotalCharges,freq=F,15)
mm <- mean(df$TotalCharges,na.rm=T)
ss <- sd(df$TotalCharges,na.rm=T)
curve(dnorm(x,mm,ss),col="red",add=T)
#shapiro.test(df$TotalCharges) #Error: too many samples for shapiro test
ad.test(df$TotalCharges) #Anderson-Darling test: Not normally distributed
Boxplot(df$TotalCharges,range=1.5,id=list(n=Inf,labels=rownames(df))) #No mild univariate outliers
Boxplot(df$TotalCharges,range=3,id=list(n=Inf,labels=rownames(df))) #No severe univariate outliers
```

# Profiling Target Variable: Churn  
We can see from the summary of the target variable Churn that it is highly unbalanced. 73% of all instances do not Churn. This may be a problem when building a model that we must keep in mind.

Using catdes we can see that the most highly related categorical variables are Contract, OnlineSecurity, TechSupport, InternetService, PaymentMethod, ONlineBackup, DeviceProtection, StreamingMovies, StreamingTV, PaperlessBilling, Dependents, SeniorCitizen, Partner, and MultipleLines. All of these variables also have an extremely low p-value (far below the 5% significance level) which indicates a strong link to the target. For the quantitative variables, we can see that the target Churn is highly linked to all the numeric variables, tenure, TotalCharges, and MonthlyCharges.
```{r}
na.churn <- sum(is.na(df$Churn)) #No NAs
summary(df$Churn)
ptt<-prop.table(table(df$Churn));ptt
barplot(table(df$Churn),col='lightblue') #Unbalanced
catdes(df,21)
```

# Correlations and Associations  
If we plot the correlations between the numeric variables, the heatmap shows that MonthlyCharges and tenure are not correlated. However, intuitively TotalCharges and MonthlyCharges are positively correlated at approximately 0.5. This makes sense as the higher your monthly charges are the higher your total charges will be. Similarly, tenure has a relatively high positive correlation with TotalCharges. Once again this makes sense because the longer you are subscribed for the higher your total overall charges will be.  

To further our analysis on the relationship between variables we make use of a function that plots mixed associations using the Chisquare p-values and CramersV, for numeric and categorical variables (this function was found at https://stackoverflow.com/questions/52554336/plot-the-equivalent-of-correlation-matrix-for-factors-categorical-data-and-mi by AntoniosK on StackOverflow). Note that to interpret the graph, the color of the cells indicate the Chisquared p-value (red meaning highly related), and the label found in the cells indicated the CramersV (1 indicated a perfect association). Interestingly it seems that most variables are related according to the Chisquared test, as most cells are deep red, except for gender which according to the Chisquared independence test does not show any significant relationship with the other variables. When looking at the CramersV (labels in the cells), we can see that TotalCharges has an almost perfect association with Churn the target, and all the other variables. Aside from TotalCharges, no other variable has a near perfect association with another variables.
```{r}
num <- which(sapply(df,is.numeric))

# Correlations
vis_cor(df[,num])

# Mixed Associations (using Chisquared pvalue and CramersV)
cat <- which(sapply(df, function(x) is.factor(x) || is.character(x)))

df_corr <- df[,-1]

# function to get chi square p value and Cramers V
f = function(x,y) {
    tbl = df %>% select(x,y) %>% table()
    chisq_pval = round(chisq.test(tbl)$p.value, 4)
    cramV = round(cramersV(tbl), 4) 
    data.frame(x, y, chisq_pval, cramV) }

# create unique combinations of column names
# sorting will help getting a better plot (upper triangular)
df_comb = data.frame(t(combn(sort(names(df_corr)), 2)), stringsAsFactors = F)

# apply function to each variable combination
df_res = map2_df(df_comb$X1, df_comb$X2, f)

# plot results
df_res %>%
  ggplot(aes(x,y,fill=chisq_pval))+
  geom_tile()+
  geom_text(aes(x,y,label=cramV), size=2)+
  scale_fill_gradient(low="red", high="yellow")+
  theme_classic()+
  theme(axis.text.x = element_text(angle = 30, hjust = 1))

# Function to find mixed associations found at:
# https://stackoverflow.com/questions/52554336/plot-the-equivalent-of-correlation-matrix-for-factors-categorical-data-and-mi
# By AntoniosK on StackOverflow
```

# Multivariate Outliers  
Using the Moutlier function at a 1% significance level we find that there are 62 outliers. Checking the summary of the outliers we see that they have abnormally high tenure, and very high TotalCharges. However, when we plot the robust distance versus the mahalanobis distance we see that there is a lot of continuity in the points. We believe that all the points so closely together, springing out in three continuous spikes means that these outliers are expected, as many other observations come close to this cutoff. Thus, we decide to keep all of the multivariate outliers. This is becuase of this strong continuity in the graph and the relatedness of all the points close to the cutoff. Losing these points mean we lose valuable information as there is a clear pattern, namely the higher the tenure the higher the overall total charges, even if they are very high numbers that are shown to be outliers by the Moutlier function. We want this relationship to be captured in the model.
```{r}
res.mout <- Moutlier(df[,num],quantile=0.99,plot=F)
length(which(res.mout$md > res.mout$cutoff))
mout <- which(res.mout$md > res.mout$cutoff)
summary(df[mout, num]) #Summary of outliers
plot(res.mout$md, res.mout$rd)

#df <- df[-m.out,] #remove multivariate outliers
```


# Modelling  
Now we enter into the modelling stage of the analysis. We first want to construct a robust numerical model, in order to add transformations in the numerical variables. Afterwards we will add the main categorical variables into this best numerical model, and finally we will look at interactions between variables to make the final model.

#  Model with numerical variables  
We only have 3 numerical variables, so, in the first place, we construct a model with all 3 numerical variables: tenure, MonthlyCharges and TotalCharges. We are suspicious of multicolinearity regarding TotalCharges, since the number calculated by multiplying the tenure months by MonthlyCharges gives a similar result than the TotalCharges value (with small a deviation probably coming from opening fees or discounts in the different contracts). (...)
```{r}
attach(df)
nm1 <- glm(Churn ~ tenure + MonthlyCharges + TotalCharges, family="binomial", data = df)
summary(nm1)
vif.nm1 <- vif(nm1);vif.nm1
step(nm1,k= log(nrow(df))) 
```

```{r}
nm2 <- glm(Churn ~ tenure + MonthlyCharges, family="binomial", data = df)
summary(nm2)
anova(nm2,nm1,test="Chisq") #p-value: 0.02277 -> we reject at 95% confidence that the variance explained is the same. We don't at 99% confidence.
```


#  Transformations of numerical variables  
Firstly, we examine marginalModelPlots to gain insights into which variables effectively fit the model. Subsequently, we observed the necessity for a transformation in the tenure variable.
A reduction of one unit in tenure is associated with a log-odds increase of -0.054850 for Churn. Consequently, implementing a Square Root Transformation becomes imperative.
```{r}
nm3 <- glm(Churn ~ tenure+ I(tenure^2) + MonthlyCharges, family="binomial", data = df)
summary(nm3)
marginalModelPlots(nm3)

nm4 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges, family="binomial", data = df)
summary(nm4)
marginalModelPlots(nm4)
```

#  NM4: Residual analysis and Influential Data  
(INTERPRETATION OF RESIDUAL PLOTS)

When looking at the hat values, using the influenceIndexPlot, we see that there are many no points that stand out relative to the rest. We have a similar case when looking at the influencePlot. We can confirm that there are no outstanding hat values by drawing a boxplot and a typical cutoff line at 4*p/n. This plot shows us there are no points with hat values above this cutoff and therefore no further action needs to be taken.

In a similar way, when we plot the influenceIndexPlot for Cooks' Distance, we find a few (approximately 5) points that stand out above the rest. To further investigate we draw a boxplot of Cooks' Distance and find that indeed there are four points that are relatively further out than others. We can draw a clear cutoff line at 0.003 to separate these points. When we further inspect these points by comparing their summary statistics against overall summaries of Tenure and Monthly Charges we find that tenure is significantly higher for these outlier observations while the average MonthlyCharges are almost half of the overall average. We decide to remove these points.
```{r}
# Residuals
residualPlots(nm4, layout=c(1, 3))

influencePlot(nm4)

# Hat values
influenceIndexPlot(nm4, id.n=10, vars=c('hat'))
Boxplot(hatvalues(nm4), ylim=c(0,0.0025))
abline(h=4*length(coef(nm4))/nrow(df))

# Cooks distance
influenceIndexPlot(nm4, id.n=10, vars=c('Cook'))
Boxplot(cooks.distance(nm4))
abline(h=0.003)
llcoo <- which(cooks.distance(nm4)> 0.003);
summary(df[,c('tenure', 'MonthlyCharges')])
summary(df[llcoo,c('tenure', 'MonthlyCharges')])

df <- df[-llcoo,]
rownames(df) <- NULL
```


#  Adding main categorical effects  
Now we attempt to add our factor variables to the model. First we are building an inicial model with the main numerical variables and all the categorical ones, which will obviously result in a too complex model to be analysed or properly interpreted. After that, we are conducting an Anova Chisq test to assess the significance of each categorical variable. 
```{r}
cm1 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges + gender + SeniorCitizen + Partner + Dependents + PhoneService + MultipleLines + InternetService + OnlineSecurity + OnlineBackup + DeviceProtection + TechSupport + StreamingTV + StreamingMovies + Contract + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(cm1)
Anova(cm1,test="LR")
```
This Anova tests suggests, at 95% confidence, to take out of the modeling the following variables: gender, Partner, Dependets, PhoneService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV and StreamingMovies. Now we construct a model only with the significant variables.
```{r}
cm2 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(cm2)
```

#  CM2: Residual Analysis and Influential Data  
From the general influencePlot we can see that there are a couple possible outliers that need further investivation such as observations 3821 and 489, which seem to have abnormally high hat values. These same points stand out in the influenceIndexPlot. When we now draw a boxplot of the hat values and draw a cutoff line at the typical cutoff 4*p/n, we see that point 489 is the only one above this cutoff, with point 2819 just below. Thus, we remove observation 489.  

When we further investigate Cooks' Distance using influenceIndexPlot we see more observations stand out with observations 3972 and 5948 being fat above the rest. Now when we draw the boxplot, we again see these two points stand out with several other points. Here there is no clear cutoff as the observations splinter into groups, where one group has a large distance to the rest, and then the two observations mentioned even further than this. Thus, to not lose too much valuable information we decide to remove only the two largest outliers, thus moving the cutoff to 0.0033. When comparing the Cooks outliers summaries to the all the data it is difficult to see where they are different. They seem to generally have lower charges (monthly and total), with longer tenure but nothing noteworthy that would suggest why they seem to be so influential from the summary statistics alone.
```{r}
# Residuals
residualPlots(cm2, layout=c(1, 3))

influencePlot(cm2)

# Hat values
influenceIndexPlot(cm2, id.n=10, vars=c('hat'))
Boxplot(hatvalues(cm2), ylim=c(0,0.01))
abline(h=4*length(coef(cm2))/nrow(df))
# hatvalues(cm2)[487] --> referring to observation 489
df <- df[-489,]
rownames(df) <- NULL

# Cooks distance
influenceIndexPlot(cm2, id.n=10, vars=c('Cook'))
Boxplot(cooks.distance(cm2))
abline(h=0.0033)
llcoo <- which(cooks.distance(cm2)> 0.0033);
#summary(df)          --> uncomment to compare
#summary(df[llcoo,])  --> uncomment to compare

df <- df[-llcoo,]
rownames(df) <- NULL
```

```{r}
cm2 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod, family="binomial", data = df)
```


#  Interactions  
Significant interactions (5% level):  
poly(tenure, 2):Contract  
MonthlyCharges:MultipleLines  
MonthlyCharges:InternetService  
MonthlyCharges:Contract  
SeniorCitizen:PaymentMethod  
InternetService:Contract  
InternetService:PaymentMethod  
```{r}
Anova(glm(Churn ~ (poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod) * (poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod), family="binomial", data = df))
```
We will be adding interactions one by one to our model, and we will see if they have significance over the last model, and, if they do, we keep them in the model and move on. Note that to do that we will be using Fisher tests, with the null hypothesis being that the variance explained by both models are the same.   
First we look into the interaction between Contract and the transformed tenure. We can see that the role it performs on the model is to ... . Then, we perform an Fisher test to see if the variances explained by the two models are the same or not. 
```{r}
gm1 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm1)
#DOES WORK
anova(cm2,gm1,test="Chisq") #p-value 2.982e-06
```
We can see that the p-value in this case is 2.982e-06, so we reject the null hypothesis and we decide to keep the interaction forward. 
Now let's look at the interactions regarding MonthlyCharges, starting by its interaction with MultipleLines.
```{r}
gm2 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*MultipleLines + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm2)
anova(gm1,gm2,test="Chisq") #p-value 0.01424 
```
In this case, the p-value is 0.01424, which would make us reject the hypothesis at 99% confidence, but accept it at 95%. We decide to take it out of the model.
```{r}
gm3 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm3)
anova(gm1,gm3,test="Chisq") #p-value 0.005535  
```
This time, the p-value is 0.005535, so we will keep the interaction in the model.
```{r}
gm4 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*(InternetService + Contract) + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm4)
anova(gm3,gm4,test="Chisq") #p-value 0.8 
```
Now the p-value is 0.8, so we accept the null hypothesis and not keep this interaction.
```{r}
gm5 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen*PaymentMethod + MultipleLines + InternetService + PaperlessBilling , family="binomial", data = df)
summary(gm5)
anova(gm3,gm5,test="Chisq") #p-value 0.035   
```
In this final case, we could reject the null hypothesis at 95% confidence, but we decide to take this interaction  out of the model.


# Train-test validation & Final interpretation  
This is the final model as it is. First of all we are going to make a residual analysis and, if there is any, take out the influential data to get the model performing at its best, before intepreting it and testing it. 
```{r}
fm <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(fm)
Anova(fm)
```

#  Residual Analysis and Influential Data  
From the influencePlot of the final model we see that there may be a lot of observations with abnormally high hat values that must be investigated, as well as some observations with extreme Cooks' Distances. When checking the influenceIndexPlot we can see many observations that stand out above the rest, for example observation 2367 and 3663. When the boxplot is plotted with the cutoff at 4*p/n we find that 56 observations are above this cutoff. We decide to remove all of these observations.  

When analyzing the Cooks' Distance using influenceIndexPlot we see only two observations that stand out far above the rest, 3969 and 5942. When we continue by drawing the boxplot we get the same result, and thus set the cutoff at 0.01 to remove these observations. Once again the analysis of the summary statistics is not very revealing, the outliers seem to have much lower charges (monthly and total), however the comparison between other factors is not adequate as there are only two outliers.
```{r}
# Residuals
residualPlots(fm, layout=c(1, 3))

influencePlot(fm)

# Hat values
influenceIndexPlot(fm, id.n=10, vars=c('hat'))
Boxplot(hatvalues(fm), ylim=c(0,0.04))
abline(h=4*length(coef(fm))/nrow(df))
llhat <- which(hatvalues(fm)>4*length(coef(fm))/nrow(df))
df <- df[-llhat,]
rownames(df) <- NULL

# Cooks distance
influenceIndexPlot(fm, id.n=10, vars=c('Cook'))
Boxplot(cooks.distance(fm))
abline(h=0.01)
llcoo <- which(cooks.distance(fm)> 0.01);
summary(df[,])
summary(df[llcoo,])

df <- df[-llcoo,]
rownames(df) <- NULL
```

#  Interpretation  
Now that we have our influential data removed, let's move on to the interpretation of the model. Let's keep in mind that the base class for this model is Churn:No. To start with the numerical variables, MonthlyCharges parameter is negative, which means that, as the amount charged by the month increases, it decreases the probability in the logodds scale that a customer will leave by 1.514e-03 units, which is not very much so we could consider it neutral. In the case of tenure, we see that as a client stays more months in the company, the probability of it to leave is 6.892e+0 units less in the logodds scale. This, a priori, speaks well of the company, since it is clear that clients tend to leave less as they experiment its services over time. However, the transformation of tenure^2 has the opposite effect, meaning that with clients that have been a lot of months in the company are more probable to leave.  

Now for the categorical variables. Looking at contract we can see that, over the base class which is month to month, customers with contracts of one year are a little less probable to leave, and even less for contracts of two years, that are 3.798e+00 units less probable to leave in the logodds scale. In the case of InternetService, No internet service and Fiber are less probable to leave than DSL. Then, we see that SeniorCitizens are more probable to leave, which maybe can be explained because they are more probable to die. Futhermore, those clients with No phone service or multiple lines are more likely to leave than those with only one phone line. The paperless bill also plays a part, since those using a paperless (online) bill are 3.838e-01 units more probable to leave in the logodds scale than those with paper bill. That can be maybe attributed to the fact that many paper bill users don't even see the bill, since paper mail is given less attention these days. In terms of payment method, MailedCheck makes it less likely for the clients to leave than Bank transfer (the base class), which has to do with what was mentioned earlier in the Paperless bill. The same phenomena is observed with CreditCard payment although it is not as pronounced. The opposite is observed with Wlectronic check, since users of this payment method are more likely to leave the company.  

Finally, let's look at the interactions. To start with, we have the interaction between the transformed tenure and Contracts. We can see that, regarding one year contracts, the probability of a customer to leave increases 2.276e+01 units in the logodds scale as tenure months increase compared to those customers with month to month contracts; and the probability of a customer to leave decreases 2.297e+01 units in the logodds scale as tenure^2 months increase. Similar with two-year contracts but not as pronounced. This means that, customers with one year or two year contracts are more likely to leave than those with month to month contracts over the months. However, the opposite happens with tenure^2, which means that if you look over large periods of time moving faster, customers with closed contracts are less likely to leave than those with month to month contracts. To end with, let's look at the interaction between MonthlyCharges and InternetService. We can see that those customers with Fiber optic are more likely to leave as their monthly charges go up than those with DSL, and the opposite with those with no internet service. However, the coeficients are relatively small, so no further conclusions must be dragged from this. 
```{r}
fm <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(fm)
```

#  Train-test  
Initially, the dataset was partitioned into two subsets, namely the training set, comprising 80% of the data, and the test set, consisting of 20%. Subsequently, the model was trained using the training data to predict churn. The next step involved predicting dropout in the test dataset. The model achieved an accuracy of 80%.  

Finally, an additional assessment was conducted using the ROC curve. The Area Under the Curve (AUC) on the Receiver Operating Characteristic (ROC) curve serves as a metric for gauging the classifier's efficacy in distinguishing between positive and negative classes. An AUC value of 0.87 suggests very robust performance. Typically values above 0.90 are not achievable in practice and therefore our model shows near excellent behaviour in this regard.
```{r}
# Install and load the pROC package
#install.packages("pROC")
library(pROC)

set.seed(1234)
llwork <- sample(1:nrow(df), round(0.8 * nrow(df), dig = 0))
train_data <- df[llwork, ]
test_data <- df[-llwork, ]

# Train model to predict churn
fm <- glm(Churn ~ poly(tenure, 2) * Contract + MonthlyCharges * InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family = "binomial", data = train_data)

# Predict churn using the model
fm_pred <- predict(fm, test_data, type = "response")

# Print first few results
head(fm_pred, 10)

# Check model performance
test_data$predict.Churn <- ifelse(fm_pred < 0.5, "No", "Yes")

# Accuracy
accuracy <- mean(test_data$predict.Churn == test_data$Churn)
cat("Accuracy:", accuracy, "\n")

# Create ROC curve
roc_curve <- roc(test_data$Churn, fm_pred)

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)

# Add AUC to the plot
auc_value <- auc(roc_curve)
legend("bottomright", legend = paste("AUC =", round(auc_value, 2)), col = "blue", lwd = 2)

```

