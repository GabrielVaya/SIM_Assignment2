---
title: "Untitled"
author: "Gabriel Vayá"
date: "2023-12-13"
output: 
  pdf_document: 
    toc: true
    toc_depth: 3
    number_sections: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

library(car)
library(MASS)
library(missMDA)
library(visdat)
library(FactoMineR)
library(chemometrics)
library(corrplot)
library(naniar)
library(nortest)
library(visdat)
library(tidyverse)
library(lsr)
library(effects)
```

First thing will be to load the data into R, and redeclaring the variables to properly comply with the needs of the analysis. Notice we relable the levels of the variable SeniorCitizen from (0,1) to (No, Yes) for practical reasons. 
```{r}
setwd("C:\\Users\\darry\\Documents\\MDS\\Statistical_Inference_And_Modelling\\SIM_Assignment2")
#setwd("/Users/gabrielvayaabad/Desktop/MDS/SIM/Assignment 2")
#setwd("C:/Users/Admin/Desktop/MÀSTER DATA SCIENCE/SIM/Assigment 2")
#setwd("D:/MDS/ADSDB/SIM_Assignment2")

df <- read.csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
#df <- WA_Fn_UseC_Telco_Customer_Churn

df$MonthlyCharges <- as.numeric(df$MonthlyCharges)
df$TotalCharges <- as.numeric(df$TotalCharges)
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
df$SeniorCitizen <- as.factor(df$SeniorCitizen)
df$customerID <- as.character(df$customerID)
levels(df$SeniorCitizen) <- c("No","Yes")
```


#DataPreparation

#Deduplication
Now we check for duplicates in the dataset, and we see all rows are distinct. 
```{r}
dup <- which(duplicated(df)) #No duplicate rows
```

#Missing Values
Null values only found in TotalCharges. However, the null values are not random, as shown by the mcar_test function, with a p-value of 0. We can see that the TotalCharges are null when tenure is 0, which makes sense intuitively thus we changed the null values in TotalCharges to 0.
```{r}
vis_miss(df)
df[is.na(df$TotalCharges),] #NAs only found in TotalCharges variable

mcar_test(df) #p-value is 0 -> not random
df$tenure[is.na(df$TotalCharges)] #TotalCharges are NA when tenure is 0
df$TotalCharges[is.na(df$TotalCharges)] <- 0 #impute with 0
```

#Deduplication (again)
No duplicate rows that need to be removed.
```{r}
dup <- which(duplicated(df)) #No duplicate rows
```

#Looking for errors
```{r}
df.aux <- df
df.aux$TheoreticalTotalCharges <- df.aux$tenure*df.aux$MonthlyCharges
df.aux[df.aux$TheoreticalTotalCharges > df.aux$TotalCharges,]
```


#EDA (+ Univariate Outliers)

#gender
No null values, and balanced.
```{r}
na.gender <- sum(is.na(df$gender)) #No NAs
barplot(table(df$gender),col='lightblue') #Balanced
```

#SeniorCitizen
No null values, but unbalanced.
```{r}
na.seniorcitizen <- sum(is.na(df$SeniorCitizen)) #No NAs
barplot(table(df$SeniorCitizen),col='lightblue') #Unbalanced
```

#Partner
No null values, and balanced.
```{r}
na.partner <- sum(is.na(df$Partner)) #No NAs
barplot(table(df$Partner),col='lightblue') #Balanced
```

#Dependents
No null values, but unbalanced.
```{r}
na.dependents <- sum(is.na(df$Dependents)) #No NAs
barplot(table(df$Dependents),col='lightblue') #Unbalanced
```

#tenure
No null values, and young customers overrepresented. Not normally distributed with no mild/severe univariate outliers.
```{r}
na.tenure <- sum(is.na(df$tenure)) #No NAs
hist(df$tenure,freq=F,15) #Young customers overrepresented
mm <- mean(df$tenure,na.rm=T);ss <- sd(df$tenure,na.rm=T);
curve(dnorm(x,mm,ss),col="red",add=T)
#shapiro.test(df$tenure) #Error: too many samples for shapiro test
ad.test(df$tenure) #Anderson-Darling test: Not normally distributed
Boxplot(df$tenure,range=1.5,id=list(n=Inf,labels=rownames(df))) #No mild univariate outliers
Boxplot(df$tenure,range=3,id=list(n=Inf,labels=rownames(df))) #No extreme univariate outliers
```

#PhoneService
No null values, but unbalanced.
```{r}
na.phoneservice <- sum(is.na(df$PhoneService)) #No NAs
barplot(table(df$PhoneService),col='lightblue') #Unbalanced
```

#MultipleLines
No null values, but unbalanced in "No phone service".
```{r}
na.multiplelines <- sum(is.na(df$MultipleLines)) #No NAs
barplot(table(df$MultipleLines),col='lightblue') #Unbalanced in "No phone service"
```

#InternetService
No null values, but unbalanced.
```{r}
na.internetservice <- sum(is.na(df$InternetService)) #No NAs
barplot(table(df$InternetService),col='lightblue') #Unbalanced
```

#OnlineSecurity
No null values, but unbalanced in "No".
```{r}
no.onlinesecurity <- sum(is.na(df$OnlineSecurity)) #No NAs
barplot(table(df$OnlineSecurity),col='lightblue') #Unbalanced in "No"
```

#OnlineBackup
No null values, but unbalanced.
```{r}
na.onlinebackup <- sum(is.na(df$OnlineBackup)) #No NAs
barplot(table(df$OnlineBackup),col='lightblue') #Unbalanced
```

#DeviceProtection
No null values, but unbalanced.
```{r}
na.deviceprotection <- sum(is.na(df$DeviceProtection)) #No NAs
barplot(table(df$DeviceProtection),col='lightblue') #Unbalanced
```

#TechSupport
No null values, but unbalanced in "No".
```{r}
na.techsupport <- sum(is.na(df$TechSupport)) #No NAs
barplot(table(df$TechSupport),col='lightblue') #Unbalanced in "No"
```

#StreamingMovies
No null values but unbalanced in "No internet service".
```{r}
na.streamingmovies <- sum(is.na(df$StreamingMovies)) #No NAs
barplot(table(df$StreamingMovies),col='lightblue') #Unbalanced in "No internet service"
```

#Contract
No null values, but unbalanced in "Month-to-month".
```{r}
na.contract <- sum(is.na(df$Contract)) #No NAs
barplot(table(df$Contract),col='lightblue') #Unbalanced in "Month-to-month"
```

#PaperlessBilling
No null values and relatively balanced.
```{r}
na.paperlessbilling <- sum(is.na(df$PaperlessBilling)) #No NAs
barplot(table(df$PaperlessBilling),col='lightblue') #Relatively balanced
```

#PaymentMethod
No null values. Very unbalanced in "Credit card (automatic")
```{r}
na.paymentmethod <- sum(is.na(df$PaymentMethod)) #No NAs
barplot(table(df$PaymentMethod),col='lightblue') #Unbalanced in "Credit card (automatic)"
```

#MonthlyCharges
No null values, not normally distributed and no mild/severe univariate outliers.
```{r}
na.monthlycharges <- sum(is.na(df$MonthlyCharges)) #No NAs
hist(df$MonthlyCharges,freq=F,15)
mm <- mean(df$MonthlyCharges,na.rm=T)
ss <- sd(df$MonthlyCharges,na.rm=T)
curve(dnorm(x,mm,ss),col="red",add=T)
#shapiro.test(df$MonthlyCharges) #Error: too many samples for shapiro test
ad.test(df$MonthlyCharges) #Anderson-Darling test: Not normally distributed
Boxplot(df$MonthlyCharges,range=1.5,id=list(n=Inf,labels=rownames(df))) #No mild univariate outliers
Boxplot(df$MonthlyCharges,range=3,id=list(n=Inf,labels=rownames(df))) #No severe univariate outliers
```

#TotalCharges
11 null values, not normally distributed and no mild/severe univariate outliers.
```{r}
na.totalcharges <- sum(is.na(df$TotalCharges)) #11 NAs imputed earlier
hist(df$TotalCharges,freq=F,15)
mm <- mean(df$TotalCharges,na.rm=T)
ss <- sd(df$TotalCharges,na.rm=T)
curve(dnorm(x,mm,ss),col="red",add=T)
#shapiro.test(df$TotalCharges) #Error: too many samples for shapiro test
ad.test(df$TotalCharges) #Anderson-Darling test: Not normally distributed
Boxplot(df$TotalCharges,range=1.5,id=list(n=Inf,labels=rownames(df))) #No mild univariate outliers
Boxplot(df$TotalCharges,range=3,id=list(n=Inf,labels=rownames(df))) #No severe univariate outliers
```

#Profiling Target Variable: Churn
We can see from the summary of the target variable Churn that it is highly unbalanced. 73% of all instances do not Churn. This may be a problem when building a model that we must keep in mind.

Using catdes we can see that the most highly related categorical variables are Contract, OnlineSecurity, TechSupport, InternetService, PaymentMethod, ONlineBackup, DeviceProtection, StreamingMovies, StreamingTV, PaperlessBilling, Dependents, SeniorCitizen, Partner, and MultipleLines. All of these variables also have an extremely low p-value (far below the 5% significance level) which indicates a strong link to the target. For the quantitative variables, we can see that the target Churn is highly linked to all the numeric variables, tenure, TotalCharges, and MonthlyCharges.
```{r}
na.churn <- sum(is.na(df$Churn)) #No NAs
summary(df$Churn)
ptt<-prop.table(table(df$Churn));ptt
barplot(table(df$Churn),col='lightblue') #Unbalanced
res.cat <- catdes(df,21)
res.cat$category
```

#Correlations and Associations
If we plot the correlations between the numeric variables, the heatmap shows that MonthlyCharges and tenure are not correlated. However, intuitively TotalCharges and MonthlyCharges are positively correlated at approximately 0.5. This makes sense as the higher your monthly charges are the higher your total charges will be. Similarly, tenure has a relatively high positive correlation with TotalCharges. Once again this makes sense because the longer you are subscribed for the higher your total overall charges will be.

To further our analysis on the relationship between variables we make use of a function that plots mixed associations using the Chisquare p-values and CramersV, for numeric and categorical variables (this function was found at https://stackoverflow.com/questions/52554336/plot-the-equivalent-of-correlation-matrix-for-factors-categorical-data-and-mi by AntoniosK on StackOverflow). Note that to interpret the graph, the color of the cells indicate the Chisquared p-value (red meaning highly related), and the label found in the cells indicated the CramersV (1 indicated a perfect association). Interestingly it seems that most variables are related according to the Chisquared test, as most cells are deep red, except for gender which according to the Chisquared independence test does not show any significant relationship with the other variables. When looking at the CramersV (labels in the cells), we can see that TotalCharges has an almost perfect association with Churn the target, and all the other variables. Aside from TotalCharges, no other variable has a near perfect association with another variables.
```{r}
num <- which(sapply(df,is.numeric))

# Correlations
vis_dat(df[,num], sort_type = FALSE)
vis_cor(df[,num])

# Mixed Associations (using Chisquared pvalue and CramersV)
cat <- which(sapply(df, function(x) is.factor(x) || is.character(x)))

df_corr <- df[,-1]

# function to get chi square p value and Cramers V
f = function(x,y) {
    tbl = df %>% select(x,y) %>% table()
    chisq_pval = round(chisq.test(tbl)$p.value, 4)
    cramV = round(cramersV(tbl), 4) 
    data.frame(x, y, chisq_pval, cramV) }

# create unique combinations of column names
# sorting will help getting a better plot (upper triangular)
df_comb = data.frame(t(combn(sort(names(df_corr)), 2)), stringsAsFactors = F)

# apply function to each variable combination
df_res = map2_df(df_comb$X1, df_comb$X2, f)

# plot results
df_res %>%
  ggplot(aes(x,y,fill=chisq_pval))+
  geom_tile()+
  geom_text(aes(x,y,label=cramV), size=2)+
  scale_fill_gradient(low="red", high="yellow")+
  theme_classic()+
  theme(axis.text.x = element_text(angle = 30, hjust = 1))

# Function to find mixed associations found at:
# https://stackoverflow.com/questions/52554336/plot-the-equivalent-of-correlation-matrix-for-factors-categorical-data-and-mi
# By AntoniosK on StackOverflow
```

#Multivariate Outliers
Using the Moutlier function at a 1% significance level we find that there are 62 outliers. Checking the summary of the outliers we see that they have abnormally high tenure, and very high TotalCharges. However, when we plot the robust distance versus the mahalanobis distance we see that there is a lot of continuity in the points. We believe that all the points so closely together, springing out in three continuous spikes means that these outliers are expected, as many other observations come close to this cutoff. Thus, we decide to keep all of the multivariate outliers. This is becuase of this strong continuity in the graph and the relatedness of all the points close to the cutoff. Losing these points mean we lose valuable information as there is a clear pattern, namely the higher the tenure the higher the overall total charges, even if they are very high numbers that are shown to be outliers by the Moutlier function. We want this relationship to be captured in the model.
```{r}
res.mout <- Moutlier(df[,num],quantile=0.99,plot=F)
length(which(res.mout$md > res.mout$cutoff))
mout <- which(res.mout$md > res.mout$cutoff)
summary(df[mout, num]) #Summary of outliers
plot(res.mout$md, res.mout$rd)

#df <- df[-m.out,] #remove multivariate outliers
```


#Modelling
Now we enter into the modelling stage of the analysis. We first want to construct a robust numerical model, in order to add transformations in the numerical variables. Afterwards we will add the main categorical variables into this best numerical model, and finally we will look at interactions between variables to make the final model.

#Model with numerical variables
We only have 3 numerical variables, so, in the first place, we construct a model with all 3 numerical variables: tenure, MonthlyCharges and TotalCharges. We are suspicious of multicolinearity regarding TotalCharges, since the number calculated by multiplying the tenure months by MonthlyCharges gives a similar result than the TotalCharges value (with small a deviation probably coming from opening fees or discounts in the different contracts). (...)
```{r}
attach(df)
nm1 <- glm(Churn ~ tenure + MonthlyCharges + TotalCharges, family="binomial", data = df)
summary(nm1)
vif.nm1 <- vif(nm1);vif.nm1
step(nm1,k= log(nrow(df))) 
```

```{r}
nm2 <- glm(Churn ~ tenure + MonthlyCharges, family="binomial", data = df)
summary(nm2)
anova(nm2,nm1,test="Chisq") #p-value: 0.02277 -> we reject at 95% confidence that the variance explained is the same. We don't at 99% confidence.
```


#Transformations of numerical variables
Firstly, we examine marginalModelPlots to gain insights into which variables effectively fit the model. Subsequently, we observed the necessity for a transformation in the tenure variable.
A reduction of one unit in tenure is associated with a log-odds increase of -0.054850 for Churn. Consequently, implementing a Square Root Transformation becomes imperative.
```{r}
nm3 <- glm(Churn ~ tenure+ I(tenure^2) + MonthlyCharges, family="binomial", data = df)
summary(nm3)
marginalModelPlots(nm3)

nm4 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges, family="binomial", data = df)
summary(nm4)
marginalModelPlots(nm4)
```

# NM4: Residual analysis and Influential Data
(INTERPRETATION OF RESIDUAL PLOTS)

When looking at the hat values, using the influenceIndexPlot, we see that there are many no points that stand out relative to the rest. We have a similar case when looking at the influencePlot. We can confirm that there are no outstanding hat values by drawing a boxplot and a typical cutoff line at 4*p/n. This plot shows us there are no points with hat values above this cutoff and therefore no further action needs to be taken.

In a similar way, when we plot the influenceIndexPlot for Cooks' Distance, we find a few (approximately 5) points that stand out above the rest. To further investigate we draw a boxplot of Cooks' Distance and find that indeed there are four points that are relatively further out than others. We can draw a clear cutoff line at 0.003 to separate these points. When we further inspect these points by comparing their summary statistics against overall summaries of Tenure and Monthly Charges we find that tenure is significantly higher for these outlier observations while the average MonthlyCharges are almost half of the overall average. We decide to remove these points.
```{r}
# Residuals
residualPlots(nm4, layout=c(1, 3))

influencePlot(nm4)

# Hat values
influenceIndexPlot(nm4, id.n=10, vars=c('hat'))
Boxplot(hatvalues(nm4), ylim=c(0,0.0025))
abline(h=4*length(coef(nm4))/nrow(df))

# Cooks distance
influenceIndexPlot(nm4, id.n=10, vars=c('Cook'))
Boxplot(cooks.distance(nm4))
abline(h=0.003)
llcoo <- which(cooks.distance(nm4)> 0.003);
summary(df[,c('tenure', 'MonthlyCharges')])
summary(df[llcoo,c('tenure', 'MonthlyCharges')])

df <- df[-llcoo,]
rownames(df) <- NULL
```


#Adding main categorical effects
Now we attempt to add our factor variables to the model. First we are building an inicial model with the main numerical variables and all the categorical ones, which will obviously result in a too complex model to be analysed or properly interpreted. After that, we are conducting an Anova Chisq test to assess the significance of each categorical variable. 
```{r}
cm1 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges + gender + SeniorCitizen + Partner + Dependents + PhoneService + MultipleLines + InternetService + OnlineSecurity + OnlineBackup + DeviceProtection + TechSupport + StreamingTV + StreamingMovies + Contract + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(cm1)
Anova(cm1,test="LR")
```
This Anova tests suggests, at 95% confidence, to take out of the modeling the following variables: gender, Partner, Dependets, PhoneService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV and StreamingMovies. Now we construct a model only with the significant variables.
```{r}
cm2 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(cm2)
```

# CM2: Residual Analysis and Influential Data
(RESIDUAL ANALYSIS)

From the general influencePlot we can see that there are a couple possible outliers that need further investivation such as observations 3821 and 489, which seem to have abnormally high hat values. These same points stand out in the influenceIndexPlot. When we now draw a boxplot of the hat values and draw a cutoff line at the typical cutoff 4*p/n, we see that point 489 is the only one above this cutoff, with point 2819 just below. Thus, we remove observation 489.

When we further investigate Cooks' Distance using influenceIndexPlot we see more observations stand out with observations 3972 and 5948 being fat above the rest. Now when we draw the boxplot, we again see these two points stand out with several other points. Here there is no clear cutoff as the observations splinter into groups, where one group has a large distance to the rest, and then the two observations mentioned even further than this. Thus, to not lose too much valuable information we decide to remove only the two largest outliers, thus moving the cutoff to 0.0033. When comparing the Cooks outliers summaries to the all the data it is difficult to see where they are different. They seem to generally have lower charges (monthly and total), with longer tenure but nothing noteworthy that would suggest why they seem to be so influential from the summary statistics alone.
```{r}
# Residuals
residualPlots(cm2, layout=c(1, 3))

influencePlot(cm2)

# Hat values
influenceIndexPlot(cm2, id.n=10, vars=c('hat'))
Boxplot(hatvalues(cm2), ylim=c(0,0.01))
abline(h=4*length(coef(cm2))/nrow(df))
# hatvalues(cm2)[487] --> referring to observation 489
df <- df[-489,]
rownames(df) <- NULL

# Cooks distance
influenceIndexPlot(cm2, id.n=10, vars=c('Cook'))
Boxplot(cooks.distance(cm2))
abline(h=0.0033)
llcoo <- which(cooks.distance(cm2)> 0.0033);
#summary(df)          --> uncomment to compare
#summary(df[llcoo,])  --> uncomment to compare

df <- df[-llcoo,]
rownames(df) <- NULL
```

```{r}
cm2 <- glm(Churn ~ poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod, family="binomial", data = df)
```


#Interactions
Significant interactions (5% level):
poly(tenure, 2):Contract
MonthlyCharges:MultipleLines
MonthlyCharges:InternetService
MonthlyCharges:Contract
SeniorCitizen:PaymentMethod
InternetService:Contract
InternetService:PaymentMethod
```{r}
Anova(glm(Churn ~ (poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod) * (poly(tenure,2) + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + Contract + PaperlessBilling + PaymentMethod), family="binomial", data = df))
```
We will be adding interactions one by one to our model, and we will see if they have significance over the last model, and, if they do, we keep them in the model and move on. Note that to do that we will be using Fisher tests, with the null hypothesis being that the variance explained by both models are the same. 
First we look into the interaction between Contract and the transformed tenure. We can see that the role it performs on the model is to ... . Then, we perform an Fisher test to see if the variances explained by the two models are the same or not. 
```{r}
gm1 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm1)
#DOES WORK
anova(cm2,gm1,test="Chisq") #p-value 2.982e-06
```
We can see that the p-value in this case is 2.982e-06, so we reject the null hypothesis and we decide to keep the interaction forward. 
Now let's look at the interactions regarding MonthlyCharges, starting by its interaction with MultipleLines.
```{r}
gm2 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*MultipleLines + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm2)
anova(gm1,gm2,test="Chisq") #p-value 0.01424 
```
In this case, the p-value is 0.01424, which would make us reject the hypothesis at 99% confidence, but accept it at 95%. We decide to take it out of the model.
```{r}
gm3 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm3)
anova(gm1,gm3,test="Chisq") #p-value 0.005535  
```
This time, the p-value is 0.005535, so we will keep the interaction in the model.
```{r}
gm4 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*(InternetService + Contract) + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(gm4)
anova(gm3,gm4,test="Chisq") #p-value 0.8 
```
Now the p-value is 0.8, so we accept the null hypothesis and not keep this interaction.
```{r}
gm5 <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen*PaymentMethod + MultipleLines + InternetService + PaperlessBilling , family="binomial", data = df)
summary(gm5)
anova(gm3,gm5,test="Chisq") #p-value 0.035   
```
In this final case, we could reject the null hypothesis at 95% confidence, but we decide to take this interaction  out of the model.

#Train-test validation & final interpretation
This is the final model as it is. First of all we are going to make a residual analysis and, if there is any, take out the influential data to get the model performing at its best, before intepreting it and testing it. 
```{r}
fm <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(fm)
Anova(fm)
```

#Residual Analysis and Influential Data
(RESIDUAL ANALYSIS)

From the influencePlot of the final model we see that there may be a lot of observations with abnormally high hat values that must be investigated, as well as some observations with extreme Cooks' Distances. When checking the influenceIndexPlot we can see many observations that stand out above the rest, for example observation 2367 and 3663. When the boxplot is plotted with the cutoff at 4*p/n we find that 56 observations are above this cutoff. We decide to remove all of these observations.

When analyzing the Cooks' Distance using influenceIndexPlot we see only two observations that stand out far above the rest, 3969 and 5942. When we continue by drawing the boxplot we get the same result, and thus set the cutoff at 0.01 to remove these observations. Once again the analysis of the summary statistics is not very revealing, the outliers seem to have much lower charges (monthly and total), however the comparison between other factors is not adequate as there are only two outliers.
```{r}
# Residuals
residualPlots(fm, layout=c(1, 3))

influencePlot(fm)

# Hat values
influenceIndexPlot(fm, id.n=10, vars=c('hat'))
Boxplot(hatvalues(fm), ylim=c(0,0.04))
abline(h=4*length(coef(fm))/nrow(df))
llhat <- which(hatvalues(fm)>4*length(coef(fm))/nrow(df))
df <- df[-llhat,]
rownames(df) <- NULL

# Cooks distance
influenceIndexPlot(fm, id.n=10, vars=c('Cook'))
Boxplot(cooks.distance(fm))
abline(h=0.01)
llcoo <- which(cooks.distance(fm)> 0.01);
summary(df[,])
summary(df[llcoo,])

df <- df[-llcoo,]
rownames(df) <- NULL
```

#Interpretation
Now that we have our influential data removed, let's move on to the interpretation of the model. Let's keep in mind that the base class for this model is Churn:No. To start with the numerical variables, MonthlyCharges parameter is negative, which means that, as the amount charged by the month increases, it decreases the probability in the logodds scale that a customer will leave by 1.514e-03 units, which is not very much so we could consider it neutral. In the case of tenure, we see that as a client stays more months in the company, the probability of it to leave is 6.892e+0 units less in the logodds scale. This, a priori, speaks well of the company, since it is clear that clients tend to leave less as they experiment its services over time. However, the transformation of tenure^2 has the opposite effect, meaning that with clients that have been a lot of months in the company are more probable to leave. 

Now for the categorical variables. Looking at contract we can see that, over the base class which is month to month, customers with contracts of one year are a little less probable to leave, and even less for contracts of two years, that are 3.798e+00 units less probable to leave in the logodds scale. In the case of InternetService, No internet service and Fiber are less probable to leave than DSL. Then, we see that SeniorCitizens are more probable to leave, which maybe can be explained because they are more probable to die. Futhermore, those clients with No phone service or multiple lines are more likely to leave than those with only one phone line. The paperless bill also plays a part, since those using a paperless (online) bill are 3.838e-01 units more probable to leave in the logodds scale than those with paper bill. That can be maybe attributed to the fact that many paper bill users don't even see the bill, since paper mail is given less attention these days. In terms of payment method, MailedCheck makes it less likely for the clients to leave than Bank transfer (the base class), which has to do with what was mentioned earlier in the Paperless bill. The same phenomena is observed with CreditCard payment although it is not as pronounced. The opposite is observed with Wlectronic check, since users of this payment method are more likely to leave the company. 

Finally, let's look at the interactions. To start with, we have the interaction between the transformed tenure and Contracts. We can see that, regarding one year contracts, the probability of a customer to leave increases 2.276e+01 units in the logodds scale as tenure months increase compared to those customers with month to month contracts; and the probability of a customer to leave decreases 2.297e+01 units in the logodds scale as tenure^2 months increase. Similar with two-year contracts but not as pronounced. This means that, customers with one year or two year contracts are more likely to leave than those with month to month contracts over the months. However, the opposite happens with tenure^2, which means that if you look over large periods of time moving faster, customers with closed contracts are less likely to leave than those with month to month contracts. To end with, let's look at the interaction between MonthlyCharges and InternetService. We can see that those customers with Fiber optic are more likely to leave as their monthly charges go up than those with DSL, and the opposite with those with no internet service. However, the coeficients are relatively small, so no further conclusions must be dragged from this. 
```{r}
fm <- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = df)
summary(fm)
```

#Test-train
```{r}
set.seed(1234)
llwork <- sample(1:nrow(df),round(0.8*nrow(df),dig=0))
train_data <- df[llwork,]
test_data <- df[-llwork,]

#train model to predict churn
fm<- glm(Churn ~ poly(tenure,2)*Contract + MonthlyCharges*InternetService + SeniorCitizen + MultipleLines + InternetService + PaperlessBilling + PaymentMethod, family="binomial", data = train_data)

#predict churn using model
fm_pred<- predict(fm, test_data, type= "response")

#print first few results
head(fm_pred,10)

#check model performance
test_data$predict.Churn <- ifelse(fm_pred<0.5, "No", "Yes")

head(test_data$Churn,5)
head(test_data$predict.Churn, 5)

accuracy <- mean(test_data$predict.Churn==test_data$Churn)
print(accuracy)
```
